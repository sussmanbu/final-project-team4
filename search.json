[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = FALSE # Use echo=FALSE or omit it to avoid code output  \n)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nRows: 212969 Columns: 86\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): iso_code, region_group, income_group, country, survey, level, cate...\ndbl (72): year, grade, comp_prim_v2_m, comp_lowsec_v2_m, comp_upsec_v2_m, co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 249 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): name, alpha-2, alpha-3, iso_3166-2, region, sub-region, intermediat...\ndbl (4): country-code, region-code, sub-region-code, intermediate-region-code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'iso_code'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'sub_region'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'sub_region'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'region'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'sub_region'. You can override using the `.groups` argument.\n`summarise()` has grouped output by 'sub_region'. You can override using the `.groups` argument.\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "prelim_eda.html",
    "href": "prelim_eda.html",
    "title": "test",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(broom)\n\n\ndata &lt;- read_csv('dataset/educ.csv')\n\nRows: 212969 Columns: 86\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): iso_code, region_group, income_group, country, survey, level, cate...\ndbl (72): year, grade, comp_prim_v2_m, comp_lowsec_v2_m, comp_upsec_v2_m, co...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncontinent &lt;- read_csv('dataset/continents2.csv')\n\nRows: 249 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): name, alpha-2, alpha-3, iso_3166-2, region, sub-region, intermediat...\ndbl (4): country-code, region-code, sub-region-code, intermediate-region-code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ncontinent &lt;- continent|&gt;\n  rename(\n    country = name\n  )\ncontinent\n\n# A tibble: 249 × 11\n   country   `alpha-2` `alpha-3` `country-code` `iso_3166-2` region `sub-region`\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;       \n 1 Afghanis… AF        AFG                    4 ISO 3166-2:… Asia   Southern As…\n 2 Åland Is… AX        ALA                  248 ISO 3166-2:… Europe Northern Eu…\n 3 Albania   AL        ALB                    8 ISO 3166-2:… Europe Southern Eu…\n 4 Algeria   DZ        DZA                   12 ISO 3166-2:… Africa Northern Af…\n 5 American… AS        ASM                   16 ISO 3166-2:… Ocean… Polynesia   \n 6 Andorra   AD        AND                   20 ISO 3166-2:… Europe Southern Eu…\n 7 Angola    AO        AGO                   24 ISO 3166-2:… Africa Sub-Saharan…\n 8 Anguilla  AI        AIA                  660 ISO 3166-2:… Ameri… Latin Ameri…\n 9 Antarcti… AQ        ATA                   10 ISO 3166-2:… &lt;NA&gt;   &lt;NA&gt;        \n10 Antigua … AG        ATG                   28 ISO 3166-2:… Ameri… Latin Ameri…\n# ℹ 239 more rows\n# ℹ 4 more variables: `intermediate-region` &lt;chr&gt;, `region-code` &lt;dbl&gt;,\n#   `sub-region-code` &lt;dbl&gt;, `intermediate-region-code` &lt;dbl&gt;\n\nmerged_data &lt;- merge(data, continent, by = \"country\")\n\nmerged_data &lt;- merged_data |&gt; rename(sub_region = `sub-region`)\n\n\nafg &lt;- data |&gt;\n  filter(iso_code == 'AFG')\nafg\n\n# A tibble: 868 × 86\n   iso_code region_group  income_group country survey  year level grade category\n   &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;   \n 1 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 2 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 3 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 4 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 5 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 6 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 7 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 8 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Ethnici…\n 9 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Location\n10 AFG      Central and … Low income … Afghan… DHS     2015 &lt;NA&gt;     NA Location\n# ℹ 858 more rows\n# ℹ 77 more variables: Sex &lt;chr&gt;, Location &lt;chr&gt;, Wealth &lt;chr&gt;, Region &lt;chr&gt;,\n#   Ethnicity &lt;chr&gt;, Religion &lt;chr&gt;, Language &lt;chr&gt;, comp_prim_v2_m &lt;dbl&gt;,\n#   comp_lowsec_v2_m &lt;dbl&gt;, comp_upsec_v2_m &lt;dbl&gt;, comp_prim_1524_m &lt;dbl&gt;,\n#   comp_lowsec_1524_m &lt;dbl&gt;, comp_upsec_2029_m &lt;dbl&gt;, eduyears_2024_m &lt;dbl&gt;,\n#   edu2_2024_m &lt;dbl&gt;, edu4_2024_m &lt;dbl&gt;, eduout_prim_m &lt;dbl&gt;,\n#   eduout_lowsec_m &lt;dbl&gt;, eduout_upsec_m &lt;dbl&gt;, comp_prim_v2_no &lt;dbl&gt;, …\n\n\n\ngrouped &lt;- data |&gt;\n  group_by(iso_code, Sex) |&gt;\n  summarise(mean = mean(comp_prim_v2_m, na.rm = TRUE), count = n())\n\n`summarise()` has grouped output by 'iso_code'. You can override using the\n`.groups` argument.\n\ngrouped\n\n# A tibble: 531 × 4\n# Groups:   iso_code [177]\n   iso_code Sex       mean count\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 AFG      Female   0.348   278\n 2 AFG      Male     0.661   279\n 3 AFG      &lt;NA&gt;     0.520   311\n 4 AGO      Female   0.462   116\n 5 AGO      Male     0.560   114\n 6 AGO      &lt;NA&gt;     0.531   142\n 7 ALB      Female   0.976   291\n 8 ALB      Male     0.973   288\n 9 ALB      &lt;NA&gt;     0.974   398\n10 ARE      Female NaN       446\n# ℹ 521 more rows\n\n\n\ngrouped2 &lt;- data |&gt;\n  group_by(iso_code) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            count = n())\ngrouped2\n\n# A tibble: 177 × 5\n   iso_code mean_primary_completi…¹ mean_educ_years_20_24 attend_higher_educat…²\n   &lt;chr&gt;                      &lt;dbl&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n 1 AFG                        0.510                  4.55                0.0784 \n 2 AGO                        0.519                  6.34              NaN      \n 3 ALB                        0.974                 10.4                 0.150  \n 4 ARE                      NaN                    NaN                 NaN      \n 5 ARG                        0.963                 11.4                 0.411  \n 6 ARM                        0.994                 10.8                 0.194  \n 7 AUS                        0.993                 12.4               NaN      \n 8 AUT                      NaN                    NaN                 NaN      \n 9 AZE                        0.975                 10.8                 0.132  \n10 BDI                        0.382                  4.26                0.00429\n# ℹ 167 more rows\n# ℹ abbreviated names: ¹​mean_primary_completion_rate, ²​attend_higher_education\n# ℹ 1 more variable: count &lt;int&gt;\n\n\n\ngrouped3 &lt;- merged_data |&gt;\n  #filter(survey == 'DHS') |&gt;\n  #filter(region_group %in% c('Central and Southern Asia', 'Europe and Northern America', 'Sub-Saharan Africa')) |&gt;\n  group_by(`sub_region`, year) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            percent_people_not_in_primary_school = mean(eduout_upsec_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            count = n()\n            ) |&gt;\n  ggplot(aes(x = year, y = percent_people_not_in_primary_school, color = sub_region)) +\n    geom_line()\n\n`summarise()` has grouped output by 'sub_region'. You can override using the\n`.groups` argument.\n\ngrouped3\n\nWarning: Removed 80 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ngrouped5 &lt;- merged_data |&gt;\n  #filter(survey == 'DHS') |&gt;\n  #filter(region_group %in% c('Central and Southern Asia', 'Europe and Northern America', 'Sub-Saharan Africa')) |&gt;\n  group_by(`sub_region`, year) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            percent_people_not_in_primary_school = mean(eduout_upsec_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            number_complete_prim_1524 = mean(comp_prim_1524_no, na.rm = TRUE),\n            count = n()\n            ) |&gt;\n  ggplot(aes(x = year, y = mean_primary_completion_rate, color = sub_region)) +\n    geom_line()\n\n`summarise()` has grouped output by 'sub_region'. You can override using the\n`.groups` argument.\n\ngrouped5\n\nWarning: Removed 63 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ngrouped6 &lt;- merged_data |&gt;\n  #filter(survey == 'DHS') |&gt;\n  #filter(region_group %in% c('Central and Southern Asia', 'Europe and Northern America', 'Sub-Saharan Africa')) |&gt;\n  group_by(region, year) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            percent_people_not_in_primary_school = mean(eduout_upsec_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            number_complete_prim_1524 = mean(comp_prim_1524_no, na.rm = TRUE),\n            count = n()\n            ) |&gt;\n  ggplot(aes(x = year, y = mean_educ_years_20_24, color = region)) +\n    geom_line()\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\ngrouped6\n\nWarning: Removed 25 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ngrouped7 &lt;- merged_data |&gt;\n  filter(sub_region %in% c('Northern Africa')) |&gt;\n  group_by(sub_region, Location) |&gt;\n  summarise(\n    mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n    count = n()\n  ) |&gt;\n  ggplot(\n    aes(x = Location, y = mean_primary_completion_rate, fill = Location) \n  )+geom_col() +ylim(c(0,1)) + labs(\n    title = 'Primary school completion rate of North Africans',\n    subtitle = 'This is data includes i) children and young people 3-5 years above primary school graduation age and ii) young people aged 15-24 years, who have completed primary school',\n    x = 'Location',\n    y = 'Primary school completion rate'\n  )\n\n`summarise()` has grouped output by 'sub_region'. You can override using the\n`.groups` argument.\n\ngrouped7\n\n\n\n\n\ngrouped8 &lt;- merged_data |&gt;\n  filter(sub_region %in% c('Sub-Saharan Africa')) |&gt;\n  group_by(sub_region, Location) |&gt;\n  summarise(\n    mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n    count = n()\n  ) |&gt;\n  ggplot(\n    aes(x = Location, y = mean_primary_completion_rate, fill = Location) \n  )+geom_col() +ylim(c(0,1)) + labs(\n    title = 'Primary school completion rate of Sub-Saharan Africans',\n    subtitle = 'This is data includes i) children and young people 3-5 years above primary school graduation age and ii) young people aged 15-24 years, who have completed primary school',\n    x = 'Location',\n    y = 'Primary school completion rate'\n  )\n\n`summarise()` has grouped output by 'sub_region'. You can override using the\n`.groups` argument.\n\ngrouped8\n\n\n\n\n\ngrouped9 &lt;- merged_data |&gt;\n  #filter(survey == 'DHS') |&gt;\n  filter(sub_region %in% c('Northern Africa')) |&gt;\n  group_by(iso_code) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            percent_people_not_in_primary_school = mean(eduout_upsec_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            number_complete_prim_1524 = mean(comp_prim_1524_no, na.rm = TRUE),\n            count = n()\n            ) |&gt;\n  ggplot(aes(x = iso_code, y = mean_primary_completion_rate, fill = iso_code)) +\n    geom_col() +ylim(c(0,1))\ngrouped9\n\n\n\n\n\ngrouped10 &lt;- merged_data |&gt;\n  #filter(survey == 'DHS') |&gt;\n  filter(sub_region %in% c('Latin America and the Caribbean')) |&gt;\n  group_by(iso_code) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            percent_people_not_in_primary_school = mean(eduout_upsec_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            number_complete_prim_1524 = mean(comp_prim_1524_no, na.rm = TRUE),\n            count = n()\n            ) |&gt;\n  ggplot(aes(x = iso_code, y = mean_primary_completion_rate, fill = iso_code)) +\n    geom_col() +ylim(c(0,1)) + theme(axis.text.x = element_text(angle = 45, hjust = 1))\ngrouped10\n\n\n\n\n\ngrouped11 &lt;- merged_data |&gt;\n  #filter(survey == 'DHS') |&gt;\n  filter(sub_region %in% c('Latin America and the Caribbean')) |&gt;\n  group_by(iso_code) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            obs_primary_completion_rate = sum(!is.na(comp_prim_v2_m)),\n            percent_people_not_in_primary_school = mean(eduout_upsec_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            number_complete_prim_1524 = mean(comp_prim_1524_no, na.rm = TRUE),\n            count = n()\n            ) |&gt;\n  ggplot(aes(x = iso_code, y = obs_primary_completion_rate, fill = iso_code)) + geom_col() + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngrouped11\n\n\n\n\n\nlm_model &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lm\")\n\ngrouped2 &lt;- data |&gt;\n  group_by(iso_code) |&gt;\n  summarise(mean_primary_completion_rate = mean(comp_prim_v2_m, na.rm = TRUE),\n            mean_educ_years_20_24 = mean(eduyears_2024_m, na.rm = TRUE),\n            attend_higher_education = mean(attend_higher_1822_m, na.rm = TRUE),\n            count = n()) |&gt;\n  ungroup()\n\ngrouped2\n\n# A tibble: 177 × 5\n   iso_code mean_primary_completi…¹ mean_educ_years_20_24 attend_higher_educat…²\n   &lt;chr&gt;                      &lt;dbl&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n 1 AFG                        0.510                  4.55                0.0784 \n 2 AGO                        0.519                  6.34              NaN      \n 3 ALB                        0.974                 10.4                 0.150  \n 4 ARE                      NaN                    NaN                 NaN      \n 5 ARG                        0.963                 11.4                 0.411  \n 6 ARM                        0.994                 10.8                 0.194  \n 7 AUS                        0.993                 12.4               NaN      \n 8 AUT                      NaN                    NaN                 NaN      \n 9 AZE                        0.975                 10.8                 0.132  \n10 BDI                        0.382                  4.26                0.00429\n# ℹ 167 more rows\n# ℹ abbreviated names: ¹​mean_primary_completion_rate, ²​attend_higher_education\n# ℹ 1 more variable: count &lt;int&gt;\n\n\n\nlm_model &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lm\")\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  fit(mean_primary_completion_rate ~ mean_educ_years_20_24, data = grouped2)\n\nlm_form_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mean_primary_completion_rate ~ mean_educ_years_20_24, \n    data = data)\n\nCoefficients:\n          (Intercept)  mean_educ_years_20_24  \n              0.12733                0.07119  \n\n\n\nsummary(lm_form_fit)\n\n             Length Class      Mode\nlvl           0     -none-     NULL\nspec          7     linear_reg list\nfit          13     lm         list\npreproc       1     -none-     list\nelapsed       1     -none-     list\ncensor_probs  0     -none-     list\n\n\n\nbroom::glance(lm_form_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.730         0.727 0.128      233. 3.54e-26     1   56.8 -108. -100.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nbroom::augment(lm_form_fit, new_data = grouped2)\n\n# A tibble: 177 × 7\n     .pred   .resid iso_code mean_primary_completion_rate mean_educ_years_20_24\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;                 &lt;dbl&gt;\n 1   0.451   0.0594 AFG                             0.510                  4.55\n 2   0.579  -0.0591 AGO                             0.519                  6.34\n 3   0.867   0.107  ALB                             0.974                 10.4 \n 4 NaN     NaN      ARE                           NaN                    NaN   \n 5   0.939   0.0237 ARG                             0.963                 11.4 \n 6   0.895   0.0996 ARM                             0.994                 10.8 \n 7   1.01   -0.0179 AUS                             0.993                 12.4 \n 8 NaN     NaN      AUT                           NaN                    NaN   \n 9   0.897   0.0790 AZE                             0.975                 10.8 \n10   0.430  -0.0486 BDI                             0.382                  4.26\n# ℹ 167 more rows\n# ℹ 2 more variables: attend_higher_education &lt;dbl&gt;, count &lt;int&gt;\n\n\n\nggplot(grouped2, aes(x = mean_educ_years_20_24, y=mean_primary_completion_rate)) + geom_point() + geom_smooth() + geom_smooth(method = 'lm', color = 'red')\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 89 rows containing non-finite values (`stat_smooth()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 89 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 89 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nggplot(\n  broom::augment(lm_form_fit, new_data =grouped2), aes(x = .pred, y = .resid)\n  ) + \n  geom_point(alpha = 0.45, style = 'O', size = 2.5) + \n  ylim(c(-1,1)) + \n  geom_line(y =   0, color = 'red')\n\nWarning in geom_point(alpha = 0.45, style = \"O\", size = 2.5): Ignoring unknown\nparameters: `style`\n\n\nWarning: Removed 89 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 89 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "MA415_team4",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "posts/2024-03-04-post-1/post-1.html",
    "href": "posts/2024-03-04-post-1/post-1.html",
    "title": "Team 4 three datasets",
    "section": "",
    "text": "Dataset 1:Weekly United States COVID-19 Racial Data By State, April 12, 2020 to March 7, 2021\nURL: https://datadryad.org/stash/dataset/doi:10.7272/Q6TT4P68\nThe dataset contains 212,969 rows and 86 columns.The dataset in question originates from a joint effort by the COVID Tracking Project and Boston University’s Center for Antiracist Research. It has been actively compiled since April 12, 2020, with updates being made twice weekly. The data is sourced from the official public health websites of various states and territories, with the collection process being carried out by volunteers, whose names have been provided. The dataset comprises 54 columns and includes 5320 data points. One notable limitation of the dataset is the inconsistency in reporting the ethnicity data across different states, particularly regarding the categorization of Hispanic or Latino races—some states include this data while others do not. There are 13 columns dedicated to documenting COVID-19 cases across different ethnicities, but there are redundancies, such as the columns for Cases_Latinx and Cases_Ethnicity_Hispanic, which appear to contain similar, if not identical, data. Additionally, the dataset includes 13 columns each for deaths, hospitalizations, and total cases, all disaggregated by ethnicity. However, these sections have many missing entries, which could pose challenges for data cleaning and further analysis.\nQuestions want to answer based on the dataset: -Does the hospitalizations rate affect the deaths rate? -Which ethnicity or group of ethnicities has the highest deaths rate? - Is there a correlation between the hospitalization rate and the death rate due to COVID-19, and if so, how strong is this relationship? -Does the hospitalization rate for COVID-19 differ significantly among ethnic groups, and what might this reveal about access to healthcare or disease severity? - How have hospitalization and death rates evolved over time for each ethnicity, and are there noticeable trends or patterns? - Are there regional variations in the death and hospitalization rates among different ethnicities, and what might this indicate about local public health policies?\nDataset 2: Ethnicity disparities in school in United States URL: https://www.education-inequalities.org/countries/united-states The dataset provided by the World Inequality Database on Education (WIDE) tracks educational disparities across various countries, including the United States. From the 212,969 rows and 86 columns, it provides data on education outcomes differentiated by wealth, gender, ethnicity, and location, among other factors. This database can be significant for analyzing racial disparities in education. Meanwhile, the dataset provides numerous variables such as income, region, and completions of education which enable us to perform variable selection to find the best variable to show the correlation between racial disparities and education. We will likely face difficulties while cleaning the data since the dataset is relatively large with numerous outliers. Question want to answer based on the dataset: -How do educational outcomes vary by region and demographic groups in the USA?\n-How have educational outcomes been affected by race in the dataset?\nDataset 3: US Police Shootings URL: https://www.kaggle.com/datasets/ahsen1330/us-police-shootings?resource=download There are 4896 rows and 15 columns for this dataset. This data was collected from Kaggle amidst the covid 19 pandemic when many hate crimes directed towards specific races were happening. The dataset is consisted of 4896 rows and 15 columns.One challenge that we may face with this dataset is that it is on the smaller side. 5,000 rows is not necessarily small, but it is definitely not large either. If we want to conduct a very reliable study, it may be worth considering finding a dataset with more rows. Another problem that we may run into is that it will be difficult to assess whether a police killing was justified or not if we do not have more description on the crime. For example, if there was a column that described how long the altercation lasted, it may provide more insight into whether or not the crime was justified.\nQuestions want to answer based on the dataset: -Does race and age play a factor in who gets killed by the police? -How are the shooting crime related to the race and age group"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-03-24-post-2/post-2.html",
    "href": "posts/2024-03-24-post-2/post-2.html",
    "title": "Post 2",
    "section": "",
    "text": "The data originates from Demographic and Health Surveys (DHS), Multiple Indicator Cluster Surveys (MICS), and other national household surveys and learning assessments across over 170 countries.The data originates from Demographic and Health Surveys (DHS), Multiple Indicator Cluster Surveys (MICS), and other national household surveys and learning assessments across over 170 countries.The data was jointly maintained and developed by the Global Education Monitoring Report and the UNESCO Institute for Statistics through their partnership.when we are trying to look at the education data from the different regions grouped, such as North America and Europe, East and Southeast Asia… We thought the grouping was not reasonable because, for example, the education systems of North American and European countries are very different, and even the countries within the European continent have distinct education systems. Thus, we may consider regrouping the data by Country.\nFor the sample population, each sample represents the data of a community in a country, and the data about the education may be categorized by different categories such as ethnicity alone, location alone, and ethnicity & location combined. There are 50 categories in total so we are able to draw maps, charts, infographics and tables from the comprehensive dataset.As mentioned above, the data are collected to achieve the goal of SDG 4 and Target 4.5 which commit to eliminate all kinds of disparities in education and provide as equitable education opportunities as possible.\nThe organization assists international governments in investing in science and technology, developing science policies, and evaluating the ways scientific research can aid in a country’s development and sustainability practices.Thus, the dataset helps support all kinds of research, and it helps making policy decisions on education, technology and all related fields to achieve the goal."
  },
  {
    "objectID": "posts/2024-03-24-post-2/post-2.html#data-background",
    "href": "posts/2024-03-24-post-2/post-2.html#data-background",
    "title": "Post 2",
    "section": "",
    "text": "The data originates from Demographic and Health Surveys (DHS), Multiple Indicator Cluster Surveys (MICS), and other national household surveys and learning assessments across over 170 countries.The data originates from Demographic and Health Surveys (DHS), Multiple Indicator Cluster Surveys (MICS), and other national household surveys and learning assessments across over 170 countries.The data was jointly maintained and developed by the Global Education Monitoring Report and the UNESCO Institute for Statistics through their partnership.when we are trying to look at the education data from the different regions grouped, such as North America and Europe, East and Southeast Asia… We thought the grouping was not reasonable because, for example, the education systems of North American and European countries are very different, and even the countries within the European continent have distinct education systems. Thus, we may consider regrouping the data by Country.\nFor the sample population, each sample represents the data of a community in a country, and the data about the education may be categorized by different categories such as ethnicity alone, location alone, and ethnicity & location combined. There are 50 categories in total so we are able to draw maps, charts, infographics and tables from the comprehensive dataset.As mentioned above, the data are collected to achieve the goal of SDG 4 and Target 4.5 which commit to eliminate all kinds of disparities in education and provide as equitable education opportunities as possible.\nThe organization assists international governments in investing in science and technology, developing science policies, and evaluating the ways scientific research can aid in a country’s development and sustainability practices.Thus, the dataset helps support all kinds of research, and it helps making policy decisions on education, technology and all related fields to achieve the goal."
  },
  {
    "objectID": "posts/2024-03-24-post-2/post-2.html#data-loading-and-cleaning",
    "href": "posts/2024-03-24-post-2/post-2.html#data-loading-and-cleaning",
    "title": "Post 2",
    "section": "Data Loading and Cleaning:",
    "text": "Data Loading and Cleaning:\nTo enhance our data preparation process, we plan to refine the regional categorization within our dataset. Initially, we identified that the ‘region’ column unexpectedly amalgamated diverse regions, such as combining Europe with North America and East with Southeast Asia. To address this, we will adopt a strategy to split the data more accurately according to geographic regions. We have sourced an additional dataset that delineates regions and continents more distinctly. By integrating this new dataset with our original data, we will reorganize the regional variables for clearer segmentation. Following these adjustments, we will conduct data visualization on the newly grouped data. This step will facilitate a deeper understanding and provide clearer insights by representing the data in a visually intuitive manner. To enhance our data preparation process, we plan to refine the regional categorization within our dataset. Initially, we identified that the ‘region’ column unexpectedly amalgamated diverse regions, such as combining Europe with North America and East with Southeast Asia. To address this, we will adopt a strategy to split the data more accurately according to geographic regions. We have sourced an additional dataset that delineates regions and continents more distinctly. By integrating this new dataset with our original data, we will reorganize the regional variables for clearer segmentation. Following these adjustments, we will conduct data visualization on the newly grouped data. This step will facilitate a deeper understanding and provide clearer insights by representing the data in a visually intuitive manner."
  },
  {
    "objectID": "posts/2024-03-24-post-2/post-2.html#data-for-equity",
    "href": "posts/2024-03-24-post-2/post-2.html#data-for-equity",
    "title": "Post 2",
    "section": "Data for Equity:",
    "text": "Data for Equity:\nThe definition of beneficence in the “Principles for Advancing Equitable Data Practice”, basically call people in data processing to maximize benefits for all human beings and avoid causing harm. In our project, we want to discuss the relationship between race and education. Recognizing our topic is full of sensitivity and complexity, we will ensure our analysis is fair without reinforcing any stereotypes to some races and regions. In order to follow beneficence, we will be meticulous in how data is interpreted and presented, and avoid making incorrect or harmful interpretations. Our data set we chose actually aligns with the principles. For example, the original data can be accessed through the websites of the Global Education Monitoring Report and the UNESCO Institute for Statistics. The website is accessible for all communities and it follows the principle of justice: return data and research results to community members in a form they can use. Concurrently, it also meets transparency, because to adhere to this principle, it would entail providing comprehensive metadata and ensuring easy access to data documentation. However, our dataset encompasses numerous variables, not all of which are pertinent to our analysis.  The presence of superfluous information poses a risk of leading to incorrect conclusions or oversimplifications. Additionally, there may be challenges due to our limited capacity to fully analyze the breadth of data available. By acknowledging these limitations and proceeding with caution, we aim to uphold the principles throughout our research."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-04-01-post-3/post-3.html",
    "href": "posts/2024-04-01-post-3/post-3.html",
    "title": "Post 3",
    "section": "",
    "text": "Data Visualizing Approach\nAfter cleaning/grouping the data, we took a deeper look to understand the data set by visualized the data in different graph to catch the pattern.\n\nThis graph shows the proportion of people in NORTH AFRICA who have completed primary school. It is interesting how people in Urban areas have a slightly higher completion rate as I would expect people in urban areas to have a lower completion rate. It is also worth noting that the NA bar is near the proportion of the Rural bar which suggests that most of the NA probably would fall into the Rural bar.\n\nThis graph is the same as the one above but for SUB-SAHARAN AFRICA. Here, overall, the proportions are lower which implies that education in sub-saharan Africa is not as good as education in Northern Africa. Also, there is a bigger difference between the urban and rural bar here.\n\nThis graph shows the mean primary completion rate for all of the North African countries.\n\nThis graph shows the mean primary school completion rate of all sub-saharan African countries. \n\nThis shows the number of observations for each Latin American country. It is definitely worth noting that countries like LCA and BRB do not have nearly as much data as the other countries. This should be noted for the entirety of our project.\n\nThis is a quick linear regression model we made. There is a relatively strong linear relationships between mean education years for 20-24 year olds and the mean primary completion rate. Note that this is grouped by country. The r-square is 0.72999 which indicated our model relative strong.\n\nThis is a residual plot. The residuals are relatively randomly distributed around y= 0, which means our model is doing a pretty good job of fitting the data. However, it is definitely worth noting that this is an extremely simple 1 variable and our models will require a lot more fine-tuning when they get more advanced."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPost 3\n\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nTeam 4\n\n\n\n\n\n\n  \n\n\n\n\nPost 2\n\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\nTeam 4\n\n\n\n\n\n\n  \n\n\n\n\nTeam 4 three datasets\n\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nTeam 4\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]